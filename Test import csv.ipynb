{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data\\\\nyc_cscl.csv'\n",
    "\n",
    "\"\"\"\n",
    "New Index\n",
    "0: PHYSICALID\n",
    "1: L_LOW_HN\n",
    "2: L_HIGH_HN\n",
    "3: R_LOW_HN\n",
    "4: R_HIGH_HN\n",
    "5: ST_LABEL\n",
    "6: BOROCODE\n",
    "7: FULL_STREE\n",
    "\"\"\"\n",
    "\n",
    "data = sc.textFile(path)\n",
    "header = data.first()\n",
    "\n",
    "out = sc.textFile(path) \\\n",
    "        .filter(lambda x: x!= header) \\\n",
    "        .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "        .filter(lambda x: len(x) >= 30) \\\n",
    "        .map(lambda x: (x[0], x[2], x[3], x[4], x[5], x[10], x[13], x[28])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547313 8\n"
     ]
    }
   ],
   "source": [
    "print(len(out), len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = sc.broadcast(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('164809', '', '', '', '', 'MITSUBISHI WILD WETLAND TRL', '2', 'MITSUBISHI WILD WETLAND TRL')\n",
      "('164809', '', '', '', '', 'MITSUBISHI WILD WETLAND TRL', '2', 'MITSUBISHI WILD WETLAND TRL')\n",
      "\n",
      "('6110', '215-001', '215-027', '215-000', '215-026', '28 AV', '4', '28 AVE')\n",
      "('6110', '215-001', '215-027', '215-000', '215-026', '28 AV', '4', '28 AVE')\n",
      "\n",
      "('145494', '317', '399', '316', '360', 'SCHERMERHORN ST', '3', 'SCHERMERHORN ST')\n",
      "('145494', '317', '399', '316', '360', 'SCHERMERHORN ST', '3', 'SCHERMERHORN ST')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for o, b in zip(out[:3], bc.value[:3]):\n",
    "    print(o)\n",
    "    print(b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('164809',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'MITSUBISHI WILD WETLAND TRL',\n",
       "  'MITSUBISHI WILD WETLAND TRL'),\n",
       " ('6110', '215-001', '215-027', '215-000', '215-026', '28 AV', '28 AVE'),\n",
       " ('145494', '317', '399', '316', '360', 'SCHERMERHORN ST', 'SCHERMERHORN ST')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_CSCL_PATH = 'data\\\\nyc_cscl.csv'\n",
    "root = 'test'\n",
    "violation_records = [os.path.join(root, 'violation_small1.csv'),\n",
    "                     os.path.join(root, 'violation_small2.csv')]\n",
    "VIOLATION_PATH = ','.join(violation_records)\n",
    "# indices for lookup table\n",
    "PHYSICALID = 0\n",
    "L_LOW_HN = 1\n",
    "L_HIGH_HN = 2\n",
    "R_LOW_HN = 3\n",
    "R_HIGH_HN = 4\n",
    "ST_LABEL = 5\n",
    "BOROCODE_IDX = 6\n",
    "FULL_STREE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_house_number(hn_record, segment):\n",
    "    # exclude single character house numbers\n",
    "    if len(hn_record) == 1 and (not hn_record.isnumeric()):\n",
    "        return False\n",
    "    # exlude cases like 789A\n",
    "    if (hn_record.find('-') == -1) and (not hn_record.isnumeric()):\n",
    "        return False\n",
    "    # if a record is empty, assigns 0\n",
    "    if len(hn_record) == 0:\n",
    "        hn_record = 0\n",
    "    # otherwise concatenate two values together\n",
    "    # example: '187-09' = 18709 <int>\n",
    "    # example: '187' = 187 <int>\n",
    "    else:\n",
    "        hn_record = int(hn_record.replace('-', ''))\n",
    "    # format house numbers in lookup segment in the same way\n",
    "    # if hn_record is even, we should use 'R'; otherwise, 'L'\n",
    "    if hn_record%2 == 0:\n",
    "        if len(segment[R_LOW_HN]) == 0:\n",
    "            lower = 0\n",
    "        else:\n",
    "            lower = int(re.sub('-0|-', '', segment[R_LOW_HN]))\n",
    "        if len(segment[R_HIGH_HN]) == 0:\n",
    "            high = 0\n",
    "        else:\n",
    "            high = int(re.sub('-0|-', '', segment[R_HIGH_HN]))\n",
    "    else:\n",
    "        if len(segment[L_LOW_HN]) == 0:\n",
    "            lower = 0\n",
    "        else:\n",
    "            lower = int(re.sub('-0|-', '', segment[L_LOW_HN]))\n",
    "        if len(segment[L_HIGH_HN]) == 0:\n",
    "            high = 0\n",
    "        else:\n",
    "            high = int(re.sub('-0|-', '', segment[L_HIGH_HN]))\n",
    "    return (lower <= hn_record) and (hn_record <= high)\n",
    "\n",
    "\n",
    "def countyname2borocode(county_name):\n",
    "    if (county_name == 'NEW Y') or (county_name == 'NEWY') or (county_name == 'NY') or (county_name == 'MH') or (county_name == 'MAN'):\n",
    "        return 1\n",
    "    elif (county_name == 'BRONX') or (county_name == 'BX'):\n",
    "        return 2\n",
    "    elif (county_name == 'KINGS') or (county_name == 'KING') or (county_name == 'K'):\n",
    "        return 3\n",
    "    elif (county_name == 'QUEEN') or (county_name == 'QU') or (county_name == 'Q'):\n",
    "        return 4\n",
    "    elif (county_name == 'R'):\n",
    "        return 5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def street_segmentid_lookup(HN, STREET_NAME, BOROCODE, physicalID_list):\n",
    "    for segment in physicalID_list:\n",
    "        street = STREET_NAME.lower()\n",
    "        # print(type(int(segment['BOROCODE'])), type(v_record['Violation County']))\n",
    "        # first check county code and street name\n",
    "        if (BOROCODE == int(segment[BOROCODE_IDX])) and \\\n",
    "           ((street == segment[FULL_STREE].lower()) or (street == segment[ST_LABEL].lower())):\n",
    "           # then, check house number: odd number is stored in left\n",
    "           if match_house_number(HN, segment):\n",
    "                return segment[PHYSICALID]\n",
    "    # returns -1 if there is no match\n",
    "    return -1\n",
    "\n",
    "\n",
    "def export_csv(output, lookup_table):\n",
    "    \"\"\" Export output in csv format \"\"\"\n",
    "    # build lookup table with counts\n",
    "    physicalIDs = {}\n",
    "    for row in lookup_table:\n",
    "        if row[PHYSICALID].isnumeric():\n",
    "            id = int(row[PHYSICALID])\n",
    "            physicalIDs.update({id:[0, 0, 0, 0, 0, 0]})\n",
    "    # assign the count in output\n",
    "    for out in output:\n",
    "        try:\n",
    "            lookup = int(out[0])\n",
    "            for idx in range(6):\n",
    "                physicalIDs[lookup][idx] = out[1][idx][1]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # export the resutl as csv\n",
    "    with open('temp.csv', 'w', newline='\\n') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for key in sorted(physicalIDs.keys()):\n",
    "            writer.writerow([key] + physicalIDs[key])\n",
    "\n",
    "\n",
    "def ols(data):\n",
    "    \"\"\" data = [(x1, y1), ..., (xi, yi), ..., (xN, yN)] \"\"\"\n",
    "    x_bar = sum([d[0] for d in data])/len(data)\n",
    "    y_bar = sum([d[1] for d in data])/len(data)\n",
    "    numerator = sum([(d[0] - x_bar)*(d[1] - y_bar) for d in data])\n",
    "    denomenator = sum([(d[0] - x_bar)**2 for d in data])\n",
    "    if denomenator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator/denomenator\n",
    "\n",
    "\n",
    "def fill_zer0(row):\n",
    "    expected = {2015: 0, 2016:0, 2017:0, 2018:0, 2019:0}\n",
    "    for x in row:\n",
    "        expected[x[0]] += x[1]\n",
    "    expected = [(k, v) for k, v in expected.items()]\n",
    "    return expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_id_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(NYC_CSCL_PATH)\n",
    "header = data.first()\n",
    "# start testing\n",
    "lookup = sc.textFile(NYC_CSCL_PATH) \\\n",
    "           .filter(lambda x: x != header) \\\n",
    "           .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "           .filter(lambda x: len(x) >= 30) \\\n",
    "           .map(lambda x: (x[0], x[2], x[3], x[4], x[5], x[10], x[13], x[28])) \\\n",
    "           .collect()\n",
    "LOOKUP_BCAST = sc.broadcast(lookup)\n",
    "# skip headers\n",
    "file = 'test\\\\violation_small.csv'\n",
    "data = sc.textFile(file)\n",
    "header = data.first()\n",
    "# load data\n",
    "res = sc.textFile(file) \\\n",
    "        .filter(lambda x: x != header) \\\n",
    "        .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "        .map(lambda x: (int(dt.datetime.strptime(x[4], '%m/%d/%Y').year), x[21], x[23], x[24])) \\\n",
    "        .filter(lambda x: (2015 <= x[0] and x[0] <= 2019)) \\\n",
    "        .map(lambda x: (x[0], countyname2borocode(x[1]), x[2], x[3])) \\\n",
    "        .filter(lambda x: x[1] > 0) \\\n",
    "        .map(lambda x: (x[0], street_segmentid_lookup(x[2], x[3], x[1], LOOKUP_BCAST.value))) \\\n",
    "        .filter(lambda x: int(x[1]) > 0) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_whole_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(NYC_CSCL_PATH)\n",
    "header = data.first()\n",
    "# start testing\n",
    "lookup = sc.textFile(NYC_CSCL_PATH) \\\n",
    "           .filter(lambda x: x != header) \\\n",
    "           .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "           .filter(lambda x: len(x) >= 30) \\\n",
    "           .map(lambda x: (x[0], x[2], x[3], x[4], x[5], x[10], x[13], x[28])) \\\n",
    "           .collect()\n",
    "LOOKUP_BCAST = sc.broadcast(lookup)\n",
    "\n",
    "file = 'test\\\\violation_small.csv'\n",
    "# to skip header\n",
    "data = sc.textFile(file)\n",
    "header = data.first()\n",
    "# start computation\n",
    "res = sc.textFile(file) \\\n",
    "        .filter(lambda x: x != header) \\\n",
    "        .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "        .filter(lambda x: len(x) >= 25) \\\n",
    "        .map(lambda x: (int(dt.datetime.strptime(x[4], '%m/%d/%Y').year), x[21], x[23], x[24])) \\\n",
    "        .filter(lambda x: (2015 <= x[0] and x[0] <= 2019)) \\\n",
    "        .map(lambda x: (x[0], countyname2borocode(x[1]), x[2], x[3])) \\\n",
    "        .filter(lambda x: x[1] > 0) \\\n",
    "        .map(lambda x: (x[0], street_segmentid_lookup(x[2], x[3], x[1], LOOKUP_BCAST.value))) \\\n",
    "        .filter(lambda x: int(x[1]) > 0) \\\n",
    "        .map(lambda x: ((x[1], x[0]), 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .sortByKey(True, 1) \\\n",
    "        .map(lambda x: (x[0][0], [(x[0][1], x[1])])) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .mapValues(lambda x: fill_zer0(x) + [('OLS_COEF', ols(x))]) \\\n",
    "        .collect()\n",
    "# count the number of total violations\n",
    "count = 0\n",
    "for segment in res:\n",
    "    for year in segment[1]:\n",
    "        if year[0] != 'OLS_COEF':\n",
    "            count += year[1]\n",
    "assert count == 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_computation_and_export_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119801\n",
      "101337\n",
      "1024\n",
      "1124\n",
      "1144\n",
      "114456\n",
      "11771\n",
      "119674\n",
      "123542\n",
      "12419\n",
      "1398\n",
      "16628\n",
      "16742\n",
      "168363\n",
      "169054\n",
      "174976\n",
      "181289\n",
      "181296\n",
      "181509\n",
      "182005\n",
      "183597\n",
      "184762\n",
      "1884\n",
      "19364\n",
      "19562\n",
      "19664\n",
      "19726\n",
      "2066\n",
      "21812\n",
      "2258\n",
      "22844\n",
      "22945\n",
      "23309\n",
      "2367\n",
      "24767\n",
      "24931\n",
      "26804\n",
      "2683\n",
      "29155\n",
      "30366\n",
      "35349\n",
      "35889\n",
      "35908\n",
      "36371\n",
      "3641\n",
      "3642\n",
      "3710\n",
      "38190\n",
      "38706\n",
      "39249\n",
      "39746\n",
      "41354\n",
      "4208\n",
      "4286\n",
      "43129\n",
      "43130\n",
      "43676\n",
      "44303\n",
      "44449\n",
      "4599\n",
      "46717\n",
      "48350\n",
      "50603\n",
      "51135\n",
      "5119\n",
      "51251\n",
      "53445\n",
      "53967\n",
      "55649\n",
      "5568\n",
      "56666\n",
      "58790\n",
      "59154\n",
      "5946\n",
      "61096\n",
      "6497\n",
      "6577\n",
      "66252\n",
      "66890\n",
      "67443\n",
      "68931\n",
      "69033\n",
      "69087\n",
      "70468\n",
      "71256\n",
      "72708\n",
      "72709\n",
      "72762\n",
      "73434\n",
      "759\n",
      "770\n",
      "78670\n",
      "79853\n",
      "79943\n",
      "80600\n",
      "82118\n",
      "82152\n",
      "86561\n",
      "9126\n",
      "91648\n",
      "91797\n",
      "92949\n",
      "95539\n",
      "9655\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'temp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-910427127faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfill_zer0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'OLS_COEF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mexport_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOOKUP_BCAST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-e4677d2e02cd>\u001b[0m in \u001b[0;36mexport_csv\u001b[1;34m(output, lookup_table)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# export the resutl as csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'temp.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphysicalIDs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'temp.csv'"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(NYC_CSCL_PATH)\n",
    "header = data.first()\n",
    "# start testing\n",
    "lookup = sc.textFile(NYC_CSCL_PATH) \\\n",
    "           .filter(lambda x: x != header) \\\n",
    "           .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "           .filter(lambda x: len(x) >= 30) \\\n",
    "           .map(lambda x: (x[0], x[2], x[3], x[4], x[5], x[10], x[13], x[28])) \\\n",
    "           .collect()\n",
    "LOOKUP_BCAST = sc.broadcast(lookup)\n",
    "# skip headers\n",
    "data = sc.textFile(VIOLATION_PATH)\n",
    "header = data.first()\n",
    "# load data\n",
    "res = sc.textFile(VIOLATION_PATH) \\\n",
    "        .filter(lambda x: x != header) \\\n",
    "        .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "        .filter(lambda x: len(x) >= 25) \\\n",
    "        .map(lambda x: (int(dt.datetime.strptime(x[4], '%m/%d/%Y').year), x[21], x[23], x[24])) \\\n",
    "        .filter(lambda x: (2015 <= x[0] and x[0] <= 2019)) \\\n",
    "        .map(lambda x: (x[0], countyname2borocode(x[1]), x[2], x[3])) \\\n",
    "        .filter(lambda x: x[1] > 0) \\\n",
    "        .map(lambda x: (x[0], street_segmentid_lookup(x[2], x[3], x[1], LOOKUP_BCAST.value))) \\\n",
    "        .filter(lambda x: int(x[1]) > 0) \\\n",
    "        .map(lambda x: ((x[1], x[0]), 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .sortByKey(True, 1) \\\n",
    "        .map(lambda x: (x[0][0], [(x[0][1], x[1])])) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .mapValues(lambda x: fill_zer0(x) + [('OLS_COEF', ols(x))]) \\\n",
    "        .collect()\n",
    "export_csv(res, LOOKUP_BCAST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]]\n",
    "df = pd.DataFrame(a)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0511c00dd526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "df.write.csv('t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not '123A'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'123A'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'123'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '123A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1].isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70-04'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s', '-', '70-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StructField, DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName('BDA Final Project') \\\n",
    "                    .master('local') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Physical ID', IntegerType(), False),\n",
    "    StructField('2015_count', IntegerType(), False),\n",
    "    StructField('2016_count', IntegerType(), False),\n",
    "    StructField('2017_count', IntegerType(), False),\n",
    "    StructField('2018_count', IntegerType(), False),\n",
    "    StructField('2019_count', IntegerType(), False),\n",
    "    StructField('OLS_coef', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (0, 0, 0, 0, 0, 0.8),\n",
    "    (0, 0, 0, 0, 1, 0.2),\n",
    "    (1, 0, 0, 0, 0, 0.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| _1| _2| _3| _4| _5| _6|\n",
      "+---+---+---+---+---+---+\n",
      "|  0|  0|  0|  0|  0|0.8|\n",
      "|  0|  0|  0|  0|  1|0.2|\n",
      "|  1|  0|  0|  0|  0|0.1|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/C:/Users/under/GitHub/Big-Data-Analytics-Final/t.csv already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1677.csv.\n: org.apache.spark.sql.AnalysisException: path file:/C:/Users/under/GitHub/Big-Data-Analytics-Final/t.csv already exists.;\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-c2ae6f4bf0e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[0;32m    930\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'path file:/C:/Users/under/GitHub/Big-Data-Analytics-Final/t.csv already exists.;'"
     ]
    }
   ],
   "source": [
    "spark_df.write.csv('t.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce lookup table size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('data\\\\nyc_cscl.csv')\n",
    "header = data.first()\n",
    "# start testing\n",
    "lookup = sc.textFile(NYC_CSCL_PATH) \\\n",
    "           .filter(lambda x: x != header) \\\n",
    "           .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "           .filter(lambda x: len(x) >= 30) \\\n",
    "           .map(lambda x: (x[0], (x[2], x[3], x[4], x[5], x[10], x[13], x[28]))) \\\n",
    "           .reduceByKey(lambda x, y: x) \\\n",
    "           .map(lambda x: (x[0], x[1][0], x[1][1], x[1][2], x[1][3], x[1][4], x[1][5], x[1][6])) \\\n",
    "           .collect()\n",
    "LOOKUP_BCAST = sc.broadcast(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119801 119801\n",
      "8 8\n"
     ]
    }
   ],
   "source": [
    "print(len(lookup), len(LOOKUP_BCAST.value))\n",
    "print(len(lookup[0]), len(LOOKUP_BCAST.value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('155363', '', '', '', '', 'BQE', '4', 'B Q E'),\n",
       " ('56796', '1', '35', '2', '34', 'THORNYCROFT AV', '5', 'THORNY CROFT AVE'),\n",
       " ('82534',\n",
       "  '115-001',\n",
       "  '115-099',\n",
       "  '115-000',\n",
       "  '115-098',\n",
       "  '148 ST',\n",
       "  '4',\n",
       "  '148 ST'),\n",
       " ('7962', '11-001', '11-009', '0', '0', '50 AV', '4', '50 AVE'),\n",
       " ('50539', '1301', '1359', '1300', '1378', 'TAYLOR AV', '2', 'TAYLOR AVE'),\n",
       " ('72929', '97', '159', '98', '160', 'WOODBINE ST', '3', 'WOODBINE ST'),\n",
       " ('97577', '14-001', '14-071', '14-000', '14-070', '156 ST', '4', '156 ST'),\n",
       " ('7136', '', '', '', '', 'GRAND CENTRAL PKWY', '4', 'GCP'),\n",
       " ('176643',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'FRANKLIN D ROOSEVELT DRIVE',\n",
       "  '1',\n",
       "  'F ROOSEVELT DR'),\n",
       " ('92124', '87', '111', '84', '110', 'HECKER ST', '5', 'HECKER ST')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\nyc_cscl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547313, 32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119801"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['PHYSICALID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'test\\\\violation_small.csv'\n",
    "# to skip header\n",
    "data = sc.textFile(file)\n",
    "header = data.first()\n",
    "# start computation\n",
    "res = sc.textFile(file) \\\n",
    "        .filter(lambda x: x != header) \\\n",
    "        .mapPartitions(lambda x: csv.reader(x)) \\\n",
    "        .filter(lambda x: len(x) >= 25) \\\n",
    "        .map(lambda x: (int(dt.datetime.strptime(x[4], '%m/%d/%Y').year), x[21], x[23], x[24])) \\\n",
    "        .filter(lambda x: (2015 <= x[0] and x[0] <= 2019)) \\\n",
    "        .map(lambda x: (x[0], countyname2borocode(x[1]), x[2], x[3])) \\\n",
    "        .filter(lambda x: x[1] > 0) \\\n",
    "        .map(lambda x: (x[0], street_segmentid_lookup(x[2], x[3], x[1], LOOKUP_BCAST.value))) \\\n",
    "        .filter(lambda x: int(x[1]) > 0) \\\n",
    "        .map(lambda x: ((x[1], x[0]), 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .sortByKey(True, 1) \\\n",
    "        .map(lambda x: (x[0][0], [(x[0][1], x[1])])) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .mapValues(lambda x: fill_zer0(x) + [('OLS_COEF', ols(x))]) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for segment in res:\n",
    "    for year in segment[1]:\n",
    "        if year[0] != 'OLS_COEF':\n",
    "            count += year[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
